{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from collections import OrderedDict\n",
    "from torchsummary import summary\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "%matplotlib inline\n",
    "from foolbox.criteria import Misclassification\n",
    "import foolbox\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as utils\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_orig = np.load('./mnist_pure/x_train.npy')\n",
    "# train_orig_labels = np.load('./mnist_pure/y_train.npy')\n",
    "# train_orig_labels = train_orig_labels[:30000]\n",
    "# #print(train_orig_labels[0])\n",
    "# train_orig = train_orig[:30000]\n",
    "# train_adv = np.load('./attack_1/adv_imgs_train.npy')\n",
    "# train_adv_labels = np.load('./attack_1/adv_labels_train.npy')\n",
    "# train_adv_labels = train_adv_labels[:30000]\n",
    "# #print(train_adv_labels[0])\n",
    "# train_adv = train_adv[:30000]\n",
    "# x_train = np.concatenate([train_orig, train_adv], axis=0)\n",
    "# y_train = np.concatenate([train_orig_labels, train_adv_labels], axis=0)\n",
    "# x_train = x_train.reshape(60000,1,28,28)\n",
    "# #y_train = y_train.reshape(60000,1,28,28)\n",
    "# print(x_train.shape)\n",
    "# print(y_train.shape)\n",
    "\n",
    "# test_orig = np.load('./mnist_pure/x_test.npy')\n",
    "# test_orig_labels = np.load('./mnist_pure/y_test.npy')\n",
    "# test_orig_labels = test_orig_labels[:5000]\n",
    "# test_orig = test_orig[:5000]\n",
    "# test_adv = np.load('./attack_1/adv_imgs_test.npy')\n",
    "# test_adv_labels = np.load('./attack_1/adv_labels_test.npy')\n",
    "# test_adv = test_adv[:5000]\n",
    "# test_adv_labels = test_adv_labels[:5000]\n",
    "# x_test = np.concatenate([test_orig, test_adv], axis=0)\n",
    "# y_test = np.concatenate([test_orig_labels, test_adv_labels], axis=0)\n",
    "# #print(y_test[0].shape)\n",
    "# x_test = x_test.reshape(10000,1,28,28)\n",
    "# #y_test = y_test.reshape(10000,1,28,28)\n",
    "# print(x_test.shape)\n",
    "# print(y_test.shape)\n",
    "# #print(y_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random(array_imgs, array_labels, value, length):\n",
    "    rand_idx = np.random.randint(value, size=length)\n",
    "    new_arr = []\n",
    "    new_arr_labels =[]\n",
    "    for i in range(0,length):\n",
    "        new_arr.append(array_imgs[rand_idx[i]])\n",
    "        new_arr_labels.append(array_labels[rand_idx[i]])\n",
    "    new_arr = np.asarray(new_arr)\n",
    "    new_arr_labels = np.asarray(new_arr_labels)\n",
    "    return new_arr, new_arr_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 28, 28)\n",
      "(60000,)\n",
      "(10000, 1, 28, 28)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "train_value = 30000\n",
    "ori_train_length = 30000\n",
    "adv_train_length = 0\n",
    "\n",
    "#Testing\n",
    "test_value = 5000\n",
    "ori_test_length = 5000\n",
    "adv_test_length = 0\n",
    "\n",
    "\n",
    "#Training\n",
    "\n",
    "train_orig = np.load('./mnist_pure/x_train.npy')\n",
    "train_orig_labels = np.load('./mnist_pure/y_train.npy')\n",
    "train_orig, train_orig_labels = random(train_orig, train_orig_labels, value=train_value, length=ori_train_length)\n",
    "\n",
    "\n",
    "train_adv = np.load('./attack_1/adv_imgs_train.npy')\n",
    "train_adv_labels = np.load('./attack_1/adv_labels_train.npy')\n",
    "train_adv, train_adv_labels = random(train_adv,train_adv_labels, value=train_value, length=adv_train_length)\n",
    "y_train = np.concatenate([train_orig_labels, train_adv_labels], axis=0)\n",
    "x_train = np.concatenate([train_orig, train_adv], axis=0)\n",
    "x_train = x_train.reshape(60000,1,28,28)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "test_orig = np.load('./mnist_pure/x_test.npy')\n",
    "test_orig_labels = np.load('./mnist_pure/y_test.npy')\n",
    "test_orig, test_orig_labels = random(test_orig, test_orig_labels, value=test_value, length=ori_test_length)\n",
    "\n",
    "\n",
    "test_adv = np.load('./attack_1/adv_imgs_test.npy')\n",
    "test_adv_labels = np.load('./attack_1/adv_labels_test.npy')\n",
    "test_adv, test_adv_labels = random(test_adv, test_adv_labels, value=test_value, length=adv_test_length)\n",
    "# y_test = np.concatenate([test_orig_labels, test_adv_labels], axis=0)\n",
    "# x_test = np.concatenate([test_orig, test_adv], axis=0)\n",
    "# x_test = x_test.reshape(10000,1,28,28)\n",
    "\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>>> total trainning batch number: 938\n",
      "==>>> total testing batch number: 157\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, target, transform=None):\n",
    "        self.data = torch.from_numpy(data).float()\n",
    "        self.target = torch.from_numpy(target).long()\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]\n",
    "        y = self.target[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "            \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "train_dataset = MyDataset(x_train, y_train)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "\n",
    "#trans = transforms.Compose([transforms.ToTensor()])\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "print ('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "print ('==>>> total testing batch number: {}'.format(len(test_loader)))\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "img, label = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Train Original MNIST Dataset\n",
    "\n",
    "# trans = transforms.Compose([transforms.ToTensor()])\n",
    "# train_set = MNIST('./data', train=True, transform=trans, download=True)\n",
    "# test_set = MNIST('./data', train=False, transform=trans, download=True)\n",
    "\n",
    "# # train_set = MNIST('./data', train=True, download=True)\n",
    "# # test_set = MNIST('./data', train=False, download=True)\n",
    "\n",
    "\n",
    "# batch_size = 64\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#                  dataset=train_set,\n",
    "#                  batch_size=batch_size,\n",
    "#                  shuffle=True)\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#                 dataset=test_set,\n",
    "#                 batch_size=batch_size,\n",
    "#                 shuffle=False)\n",
    "\n",
    "# print ('==>>> total trainning batch number: {}'.format(len(train_loader)))\n",
    "# print ('==>>> total testing batch number: {}'.format(len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(nn.Module):          \n",
    "     \n",
    "    def __init__(self):     \n",
    "        super(LeNet5, self).__init__()\n",
    "        # Convolution (In LeNet-5, 32x32 images are given \n",
    "        # as input. Hence padding of 2 is done below)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, \n",
    "                                     kernel_size=5, stride=1, padding=2)\n",
    "        self.max_pool_1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, \n",
    "                                     kernel_size=5, stride=1, padding=2)\n",
    "        self.max_pool_2 = nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, \n",
    "                                     kernel_size=5, stride=1, padding=2)\n",
    "        self.fc1 = nn.Linear(7*7*120, 120)\n",
    "        # convert matrix with 16*5*5 (= 400) features to a matrix of 120 features (columns)\n",
    "        self.fc2 = nn.Linear(120, 84)       \n",
    "        # convert matrix with 120 features to a matrix of 84 features (columns)\n",
    "        self.fc3 = nn.Linear(84, 10)        \n",
    "        # convert matrix with 84 features to a matrix of 10 features (columns)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # convolve, then perform ReLU non-linearity\n",
    "        x = F.relu(self.conv1(x))  \n",
    "        # max-pooling with 2x2 grid \n",
    "        x = self.max_pool_1(x) \n",
    "        # Conv2 + ReLU\n",
    "        x = F.relu(self.conv2(x))\n",
    "        # max-pooling with 2x2 grid\n",
    "        x = self.max_pool_2(x)\n",
    "        # Conv3 + ReLU\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = x.view(-1, 7*7*120)\n",
    "        # FC-1, then perform ReLU non-linearity\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # FC-2, then perform ReLU non-linearity\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # FC-3\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 6, 28, 28]             156\n",
      "         MaxPool2d-2            [-1, 6, 14, 14]               0\n",
      "            Conv2d-3           [-1, 16, 14, 14]           2,416\n",
      "         MaxPool2d-4             [-1, 16, 7, 7]               0\n",
      "            Conv2d-5            [-1, 120, 7, 7]          48,120\n",
      "            Linear-6                  [-1, 120]         705,720\n",
      "            Linear-7                   [-1, 84]          10,164\n",
      "            Linear-8                   [-1, 10]             850\n",
      "================================================================\n",
      "Total params: 767,426\n",
      "Trainable params: 767,426\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 2.93\n",
      "Estimated Total Size (MB): 3.05\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = LeNet5()\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "\n",
    "summary(model, (1, 28, 28))\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.289282\n",
      "\n",
      "Test set: Average loss: 0.1509, Accuracy: 9525/10000 (95.00%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.139179\n",
      "\n",
      "Test set: Average loss: 0.0675, Accuracy: 9783/10000 (97.00%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.082481\n",
      "\n",
      "Test set: Average loss: 0.0513, Accuracy: 9828/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.029644\n",
      "\n",
      "Test set: Average loss: 0.0530, Accuracy: 9837/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.003122\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9849/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.043508\n",
      "\n",
      "Test set: Average loss: 0.0582, Accuracy: 9828/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.103520\n",
      "\n",
      "Test set: Average loss: 0.0592, Accuracy: 9841/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.001338\n",
      "\n",
      "Test set: Average loss: 0.0497, Accuracy: 9871/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.002713\n",
      "\n",
      "Test set: Average loss: 0.0623, Accuracy: 9849/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.001188\n",
      "\n",
      "Test set: Average loss: 0.0554, Accuracy: 9856/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.003588\n",
      "\n",
      "Test set: Average loss: 0.0673, Accuracy: 9842/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.025114\n",
      "\n",
      "Test set: Average loss: 0.0629, Accuracy: 9875/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.016697\n",
      "\n",
      "Test set: Average loss: 0.0513, Accuracy: 9855/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.009852\n",
      "\n",
      "Test set: Average loss: 0.0490, Accuracy: 9873/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.000014\n",
      "\n",
      "Test set: Average loss: 0.0529, Accuracy: 9870/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.002736\n",
      "\n",
      "Test set: Average loss: 0.0624, Accuracy: 9864/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.000137\n",
      "\n",
      "Test set: Average loss: 0.0665, Accuracy: 9851/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.000110\n",
      "\n",
      "Test set: Average loss: 0.0574, Accuracy: 9869/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.005524\n",
      "\n",
      "Test set: Average loss: 0.0577, Accuracy: 9886/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.000020\n",
      "\n",
      "Test set: Average loss: 0.0553, Accuracy: 9875/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 21 [0/60000 (0%)]\tLoss: 0.000380\n",
      "\n",
      "Test set: Average loss: 0.0571, Accuracy: 9873/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 22 [0/60000 (0%)]\tLoss: 0.000027\n",
      "\n",
      "Test set: Average loss: 0.0646, Accuracy: 9853/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 23 [0/60000 (0%)]\tLoss: 0.000114\n",
      "\n",
      "Test set: Average loss: 0.0662, Accuracy: 9857/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 24 [0/60000 (0%)]\tLoss: 0.006261\n",
      "\n",
      "Test set: Average loss: 0.0483, Accuracy: 9889/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 25 [0/60000 (0%)]\tLoss: 0.000074\n",
      "\n",
      "Test set: Average loss: 0.0536, Accuracy: 9877/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 26 [0/60000 (0%)]\tLoss: 0.000325\n",
      "\n",
      "Test set: Average loss: 0.0573, Accuracy: 9878/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 27 [0/60000 (0%)]\tLoss: 0.022776\n",
      "\n",
      "Test set: Average loss: 0.0624, Accuracy: 9874/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 28 [0/60000 (0%)]\tLoss: 0.000051\n",
      "\n",
      "Test set: Average loss: 0.0555, Accuracy: 9880/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 29 [0/60000 (0%)]\tLoss: 0.000793\n",
      "\n",
      "Test set: Average loss: 0.0568, Accuracy: 9868/10000 (98.00%)\n",
      "\n",
      "Train Epoch: 30 [0/60000 (0%)]\tLoss: 0.000306\n",
      "\n",
      "Test set: Average loss: 0.0566, Accuracy: 9897/10000 (98.00%)\n",
      "\n",
      "Model Saved Successfully . . .\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if use_cuda==True:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #Variables in Pytorch are differenciable. \n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #This will zero out the gradients for this batch. \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # Calculate the loss The negative log likelihood loss. \n",
    "        # It is useful to train a classification problem with C classes.\n",
    "        loss = F.nll_loss(output, target)\n",
    "        #dloss/dx for every Variable \n",
    "        loss.backward()\n",
    "        #to do a one-step update on our parameter.\n",
    "        optimizer.step()\n",
    "        #Print out the loss periodically. \n",
    "        if batch_idx % 4000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data))\n",
    "            \n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if use_cuda==True:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        with torch.no_grad(): # volatile was removed and now \n",
    "            # has no effect. Use `with torch.no_grad():` instead.\n",
    "            data= Variable(data)\n",
    "        target = Variable(target)\n",
    "        output = model(data)\n",
    "        # sum up batch loss # size_average and reduce args will \n",
    "        # be deprecated, please use reduction='sum' instead.\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').data \n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1] \n",
    "        correct += pred.eq(target.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test()\n",
    "torch.save(model.state_dict(), './models/lenet5_mnist_0.5.pt')\n",
    "print(\"Model Saved Successfully . . .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
